---
title: "ADS-506 Team 1 Final Project"
author: "Graham Ward, Jun Clemente, & Sasha Libolt"
format: pdf
editor: visual
---

```{r import}
#| message: false
#| warning: false
library(tidyverse)
library(fpp3)
library(gt)
library(tseries)
library(skimr)
library(scales)
library(here)
library(gridExtra)
```

# Exploratory Data Analysis

## Quick Summary

```{r import_dataset}
df <- read_csv(here("datasets/call_original.csv"))
# remove spaces from column names
colnames(df) <- gsub(" ", "", colnames(df))
head(df)
```

```{r quick_summary}
# quick summary of dataframe
summary(df)
```

## Columns with missing values

```{r missing_values}
# show columns with missing values
sapply(df, function(x) sum(is.na(x)))
# show rows that have missing values
rows_with_na <- df[!complete.cases(df),]
rows_with_na
# only one row missing values. remove row from dataset
df_clean <- na.omit(df)
df_clean[!complete.cases(df_clean),]
```

```{r na_or_inf}
# check for rows with NA, Inf, or -Inf
rows_with_non_finite <- df_clean %>%
  filter(
    if_any(c(HoldTime, TimeInteracting, WaitTime, WrapUpTime), ~ !is.finite(.))
  )
rows_with_non_finite
```

## Detailed view of data

```{r data_details}
skim(df_clean)
```

### Observation

The dataset has 8 features and 275,655 records.

Two of the features are datetime information.

Two features are categorical. Four features are continuous.

Out of all the records, only one row is missing data.

## Categorical Variables

```{r com_type}
df_clean %>%
  count(CommunicationType) %>%
  ggplot(aes(x = CommunicationType, y = n)) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = n), vjust = -0.5) +
  labs(
    title = "Frequency of Communication Type by Category",
    x = "Communication Type", 
    y = "Count"
  ) +
  scale_y_continuous(labels = comma) +
  theme_minimal()

df_clean %>%
  count(SubCommunicationType) %>%
  ggplot(aes(x = SubCommunicationType, y = n)) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = n), vjust = -0.5) +
  labs(
    title = "Frequency of Sub Communication Type by Category",
    x = "Communication Type", 
    y = "Count"
  ) +
  scale_y_continuous(labels = comma) +
  theme_minimal()

```

### Observations

Most communication type is by phone. Dataset contains mostly inbound communication.

## Continuous Variables

```{r boxplot_contvar}
# reshape data to long format
df_long <- df_clean %>%
  pivot_longer(
    cols = c(WaitTime, TimeInteracting, HoldTime, WrapUpTime), 
    names_to = "Variable", values_to = "Value"
  ) %>%
  mutate(Value = Value / 60)

# create box plots
ggplot(df_long, aes(x = "", y = Value)) + 
  geom_boxplot() + 
  facet_wrap(~ Variable, scales = "free_y") + 
  coord_cartesian(ylim = c(0, 5)) +
  labs(
    title = "Distribution of Continuous Variables", 
    x = "Variable", 
    y = "Wait Time (minutes)"
  ) +
  scale_y_continuous( labels = comma ) +
  theme_minimal()

ggplot(df_long, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  facet_wrap(~ Variable, scales = "free_x") + 
  labs(
    title = "Distribution",
    x = "Time (minutes)", 
    y = "Frequency"
  ) +
  scale_y_continuous(labels = comma) + 
  scale_x_continuous(labels = comma) + 
  theme_minimal()
```

```{r stats_contvar}
continuous_var <- c("WaitTime", "TimeInteracting", "HoldTime", "WrapUpTime")

skim(df_clean[, continuous_var]) %>%
  select(skim_variable, numeric.mean, numeric.sd, numeric.p0, numeric.p25, numeric.p50, numeric.p75, numeric.p100) %>%
  gt() %>%
  fmt_number(
    columns = everything(),
    decimals = 2
  ) %>%
  cols_label(
    skim_variable = "Variable",
    numeric.mean = "Mean", 
    numeric.sd = "SD", 
    numeric.p0 = "Min", 
    numeric.p25 = "25%", 
    numeric.p50 = "50%", 
    numeric.p75 = "75%", 
    numeric.p100 = "Max"
  ) %>%
  tab_header(
    title = "Statistics for Continuous Variables (Minutes)"
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(everything())
  ) %>%
  cols_align(
    align = "center", 
    columns = c(numeric.mean, numeric.sd, numeric.p0, numeric.p25, numeric.p50, numeric.p75, numeric.p100)
  )

```

### Observations

Each of the continuous variables have relatively low means and they also contain extremely high outliers.

## Time Series

### Hourly

```{r hourly}
# aggregate to hourly
df_hourly <- df_clean %>%
  mutate(hour = floor_date(StartTime, "hour")) %>%
  group_by(hour) %>%
  summarise(total_calls = n())

# convert to tsibble
df_hourly_ts <- df_hourly %>%
  as_tsibble(index = hour)

# plot using autoplot
autoplot(df_hourly_ts, total_calls) +
  labs(
    title = "Total Calls by Hour over Time", 
    x = "Hour",
    y = "Total Calls"
  ) +
  theme_minimal()

# distribution of calls by hour
ggplot(df_hourly, aes(x = hour, y = total_calls)) + 
  geom_bar(stat = "identity") +
  labs(
    title = "Distribution of Calls by Hour", 
    x = "Hour", 
    y = "Total Calls"
  )
```

#### Observations

Time series at the hourly granularity is too noisy and visually cluttered. Better information could be gathered at a lower frequency: daily, weekly, or monthly.

```{r hour_of_day}
# aggregate to hour of day
df_hour_of_day <- df_clean %>%
  mutate(hour_of_day = format(StartTime, "%H")) %>%
  group_by(hour_of_day) %>%
  summarise(total_calls = n())

# plot histogram of total counts
ggplot(df_hour_of_day, aes(x = hour_of_day, y = total_calls)) +
  geom_bar(stat = "identity", fill = "steelblue") + 
  labs(
    title = "Distribution of Calls by Hour of Day", 
    x = "Hour of Day", 
    y = "Total Calls"
  ) +
  theme_minimal()

# create df for median calls per hour
df_daily_hourly_calls <- df_clean %>%
  mutate(date = as.Date(StartTime), 
         hour_of_day = format(StartTime, "%H")) %>%
  group_by(date, hour_of_day) %>%
  summarise(total_calls = n(), .groups = 'drop')

# create df to calc median calls/hour
df_hourly_median <- df_daily_hourly_calls %>%
  group_by(hour_of_day) %>%
  summarise(median_calls = median(total_calls), .groups = 'drop')

# plot median calls by hour
ggplot(df_hourly_median, aes(x = hour_of_day, y = median_calls)) + 
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(
    title = "Median Number of Calls by Hour of Day", 
    x = "Hour of Day", 
    y = "Median Calls"
  ) + 
  theme_minimal()
```

#### Observations

Call volumes are greater than 15 calls/hour from 11am - 9pm.

### Daily

```{r daily}
# aggregate to daily
df_daily_calls <- df_clean %>%
  mutate(date = as.Date(StartTime)) %>%
  group_by(date) %>%
  summarise(total_calls = n(), .groups = 'drop')

# convert to tsibble
df_daily_calls_ts <- df_daily_calls %>%
  as_tsibble(index = date)

# plot time series of daily call vols
df_daily_calls_ts %>%
  autoplot(total_calls) + 
  labs(
    title = "Daily Call Volumes (Mar 2022 - Oct 2024)",
    y = "Total Calls",
    x = "Date"
  )

# autocorrelation
df_daily_calls_ts %>%
  fill_gaps(total_calls = 0) %>%
  ACF(total_calls) %>%
  autoplot()

# decomp of daily total call volume
decomp_daily_calls <- df_daily_calls_ts %>%
  fill_gaps(total_calls = 0) %>%
  model(stl = STL(total_calls ~ season(window = "periodic")))

# extract and view decomp components
components_calls_daily <- decomp_daily_calls %>%
  components()

# plot decomp
components_calls_daily %>%
  autoplot() +
  labs(
    title = "STL Decomposition of Daily Call Volumes", 
    y = "Total Calls", 
    x = "Date"
  )
```

#### Observations

These plots suggest a seasonal pattern in call volumes.

The ACF plot suggests strong autocorrelation with weekly seasonality as seen in the spikes every 7 days.

```{r day_of_wk_dist}
# create var for day of week order
days_of_week_order = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")

# aggregate median calls by date 
df_median_calls_by_day <- df_daily_calls %>%
  mutate(day_of_week = weekdays(date)) %>%
  group_by(day_of_week) %>%
  summarise(median_calls = median(total_calls), .groups = 'drop')

# factor to ensure proper day of week order
df_median_calls_by_day$day_of_week <- factor(
  df_median_calls_by_day$day_of_week, 
  levels = days_of_week_order
)

# 
df_median_calls_by_day %>%
  ggplot(aes(x = day_of_week, y = median_calls)) + 
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(
    title = "Median Number of Calls by Day of the Week", 
    x = "Day of the Week", 
    y = "Median Calls"
  ) +
  theme_minimal()
```

```{r day_vs_hour}
# df group by day of week and hour of day
df_day_hour_calls <- df_clean %>%
  mutate(day_of_week = weekdays(as.Date(StartTime)),
         hour_of_day = format(StartTime, "%H")) %>%
  group_by(day_of_week, hour_of_day) %>%
  summarise(total_calls = n(), .groups = 'drop')

# factor for day of week order
df_day_hour_calls$day_of_week <- factor(
  df_day_hour_calls$day_of_week,
  levels = days_of_week_order
)

# plot heatmap
df_day_hour_calls %>%
  ggplot(aes(x = hour_of_day, y = day_of_week, fill = total_calls)) +
  geom_tile() +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(
    title = "Heat Map of Call Volumes by Day of Week and Hour of Day", 
    x = "Hour of Day", 
    y = "Day of Week", 
    fill = "Total Calls"
  ) +
  theme_minimal()
```

#### Observations

Call volumes are highest, exceeding 300 calls per day from Tuesday through Friday and mostly concentrated around 1300 hrs.

### Weekly - Total Call Volume

```{r weekly}
df_weekly <- df_clean %>%
  mutate(week = floor_date(as.Date(StartTime), "week")) %>%  # Round StartTime to the beginning of the week
  group_by(week) %>%
  summarise(total_calls = n()) %>%  # Count the number of rows (calls) per week
  ungroup() %>%
  # Fill in missing weeks with 0 calls
  complete(week = seq.Date(min(week), max(week), by = "week"), fill = list(total_calls = 0))  

df_weekly_ts <- df_weekly %>%
  as_tsibble(index = week)

# plot chart
df_weekly_ts %>%
  autoplot(total_calls) +
  labs(
    title = "Weekly Call Volumes (Mar 2022 - Oct 2024)",
    y = "Total Calls",
    x = "Date"
  ) + 
  theme_minimal()

# autocorrelation
df_weekly_ts %>%
  ACF(total_calls) %>%
  autoplot()

# decomp of weekly total call volume
decomp_calls <- df_weekly_ts %>%
  fill_gaps(total_calls = 0) %>%
  model(stl = STL(total_calls ~ season(window = "periodic")))

# extract and view decomp components
components_calls <- decomp_calls %>%
  components()

# plot decomp
components_calls %>%
  autoplot() +
  labs(
    title = "STL Decomposition of Weekly Call Volumes", 
    y = "Total Calls", 
    x = "Date"
  ) +
  theme_minimal()
```

#### Observations

These chart shows that weekly call volumes may be seasonal. The ACF chart suggests there is significant positive autocorrelation.

The Remainder chart does not appear to be random. This would need to be explored further to determine if there are any uncaptured season trends.

### Monthly

```{r}
# aggregate by month
df_monthly_calls <- df_clean %>%
  mutate(month = floor_date(as.Date(StartTime), "month")) %>%
  group_by(month) %>%
  summarise(total_calls = n(), .groups = 'drop')

# convert to tsibble
df_monthly_calls_ts <- df_monthly_calls %>%
  as_tsibble(index = month)

# plot
df_monthly_calls_ts %>%
  autoplot(total_calls) +
  labs(
    title = "Monthly Call Volumes (Mar 2022 - Oct 2024)", 
    y = "Total Calls", 
    x = "Date"
  ) + 
  theme_minimal() 

# plot autocorrelation
df_monthly_calls_ts %>%
  fill_gaps(total_calls = 0) %>%
  ACF(total_calls) %>%
  autoplot() + 
  labs(
    title = "ACF of Monthly Call Volumes Time Series",
    y = "Autocorrelation"
  )

# decomp of weekly total call volume
decomp_calls_monthly <- df_monthly_calls_ts %>%
  fill_gaps(total_calls = 0) %>%
  model(stl = STL(total_calls ~ season(window = "periodic")))

# extract and view decomp components
components_calls_monthly <- decomp_calls_monthly %>%
  components()

# plot decomp
components_calls_monthly %>%
  autoplot() +
  labs(
    title = "STL Decomposition of Monthly Call Volumes", 
    y = "Total Calls", 
    x = "Date"
  ) +
  theme_minimal()

```

#### Observations

Monthly call volumes appear to have seasonal pattern. The ACF chart shows that there are no lags outside of the significance threshold indicating low autocorrelation.

```{r}
# aggregate by month
df_median_calls_by_month <- df_daily_calls %>%
  mutate(month = month(date, label = TRUE, abbr = FALSE)) %>%
  group_by(month) %>%
  summarise(median_calls = median(total_calls), .groups = "drop")

# plot 
df_median_calls_by_month %>%
  ggplot(aes(x = month, y = median_calls)) + 
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(
    title = "Median Calls by Month", 
    x = "Month", 
    y = "Median Calls"
  ) + theme_minimal()
```

#### Observations

Median call volumes \> 300 call occur between April - August.

### Weekly - Average of WaitTime

```{r}
df_weekly_wait <- df_clean %>%
  mutate(week = floor_date(as.Date(StartTime), "week")) %>%
  group_by(week) %>%
  summarise(avg_wait_time = mean(WaitTime, na.rm = TRUE)) %>%
  ungroup() %>%
  # Fill in missing weeks
  complete(week = seq.Date(min(week), max(week), by = "week"), fill = list(avg_wait_time = 0))

# convert to tsibble
df_weekly_wait_ts <- df_weekly_wait %>%
  as_tsibble(index = week)

# plot chart
df_weekly_wait_ts %>%
  autoplot(avg_wait_time) +
  labs(
    title = "Weekly Average Wait Time (Mar 2022 - Oct 2024)", 
    y = "Average Wait Time (min)",
    x = "Date"
  ) +
  theme_minimal()

# autocorrelation
df_weekly_wait_ts %>%
  fill_gaps(avg_wait_time = 0) %>%
  ACF(avg_wait_time) %>%
  autoplot() + 
  labs(
    title = "ACF of Weekly Average Wait Time", 
    y = "ACF"
  )

# decomposition
decomp_wait <- df_weekly_wait_ts %>%
  fill_gaps() %>%
  mutate(avg_wait_time = if_else(is.na(avg_wait_time), mean(avg_wait_time, na.rm = TRUE), avg_wait_time)) %>%
  model(stl = STL(avg_wait_time ~ season(window = "periodic")))

# extract and view decomp components
components_wait <- decomp_wait %>%
  components()

# plot decomp
components_wait %>%
  autoplot() + 
  labs(
    title = "STL Decomposition of Weekly Average Wait Time", 
    y = "Average Wait Time", 
    x = "Date"
  ) +
  theme_minimal()
```

#### Observations

Based on average wait time, this does not have any seasonality or cyclic elements. Some weeks were missing data and therefore arbitrarily imputed with the mean wait time.

# Data Preparation

Step 1: Drop all rows that are not an inbound phone call. 23, 185 rows dropped

```{r filter_phone}
df_phone <- df %>%
  filter(CommunicationType == "phone") %>%
  filter(SubCommunicationType == "inbound")
  dim(df_phone)
```

Step 2: Check min/max dates. Final data should be 4/11/22 - 10/31/24 as prior to 4/11/22 was on boarding the phone system and not representative of operations.

```{r min_max_dates}
max_date <- max(df_phone$StartTime)
min_date <- min(df_phone$StartTime)
print(max_date)
print(min_date)
```

```{r drop_dates}
df_date <- df_phone %>%
  filter(StartTime >= as.POSIXct("2022-04-11"))
phone_min <- min(df_date$StartTime)
print(phone_min)
```

Step 3: Separate time stamp into date, time, and day of week

```{r convert_dates}
df_date <- df_date %>%
  mutate(
    Date = as.Date(StartTime),
    Day = weekdays(StartTime)
  )
```

Step 4: Drop unnecessary columns. Since this forecast is focusing on inbound calls, we will drop the details around call length, hold times, etc. Additionally we extracted our dates, so we will drop StartTime

```{r drop_columns}
df_columns <- df_date %>%
  select(-StartTime, -EndTime, -CommunicationType, -SubCommunicationType, -WaitTime, -TimeInteracting, -HoldTime, -WrapUpTime)
```

Step 5: Add weather. Source: [https://www.ncei.noaa.gov/access/search/data-search](https://www.ncei.noaa.gov/access/search/data-search/daily-summaries?bbox=33.014,-117.462,32.418,-116.866&pageNum=1)

```{r import_weather}
weather <- read_csv(here("datasets/weather.csv"), col_types = cols(
   DATE = col_date(format = "%Y-%m-%d"),  
  TMAX = col_integer(),
  TMAX_ATTRIBUTES = col_character()
))
head(weather)
```

```{r convert_tmax}

weather <- weather %>%
  mutate(
    TMAX_CEL = as.numeric(gsub(",", "", TMAX)) / 10
  )
head(weather)
```

```{r join_weather}
df_weather <- df_columns %>%
  left_join(weather %>% select(DATE, TMAX_CEL), by = c("Date" = "DATE"))
```

```{r check_nulls}
na_count <- sapply(df_weather, function(x) sum(is.na(x)))
print(na_count)
```

Step 6: Sum up by day

```{r summarize_day}
df_prepped <- df_weather %>%
  group_by(Date, Day, TMAX_CEL) %>%
  summarise(total_calls = n(), .groups = "drop")
```

```{r save_output}
write_csv(df_prepped, here("datasets/calls_prepped.csv"))
```

# New Data Exploration

```{r scatter_plots}
temp_scatter <- df_prepped|>
  ggplot(aes(x = total_calls, y =TMAX_CEL)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "Scatter plot, Temperature and Call Volume")+ 
  geom_smooth(method = lm)

day_box <- df_prepped|>
  ggplot(aes(x = Day, y = total_calls)) +
  geom_boxplot(fill = "lightblue", color = "darkblue") +
  labs(title = "Total Calls by Day of the Week", x = "Day of the Week", y = "Total Calls") +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

grid.arrange(temp_scatter, day_box, ncol = 2) 
```

# Modeling

```{r}
# make a modeling df from the prepped df for redundancy purposes
model_df <- df_prepped
#class(model_df$Date)

# convert df to tsibble
model_ts <- model_df |>
  as_tsibble(index = Date)

#head(model_df)
```

## Data Partitioning

```{r}
# Partition the dataset into training and validation set
# Forecast horizon is 30 days, meaning validation = 30 days

# set split date
split_date <- as.Date("2024-10-01")

tng_df <- model_ts |>
  filter(Date < split_date)

#head(tng_df, 5)
#tail(tng_df, 5)

validation_df <- model_ts |>
  filter(Date >= split_date)

#head(validation_df, 5)
#tail(validation_df, 5)
```

```{r}
# initialize empty performance metrics tibble to store results
performance_metrics <- tibble(
  Model = character(),
  RMSE = numeric(),
  MAE = numeric(),
  MAPE = numeric()
)
```

### Seasonal Naive

```{r}
# fit the model to training data
snaive_model <- tng_df |>
  model(SNAIVE(total_calls ~ lag(7)))

# forecast validation period
snaive_forecast <- snaive_model |>
  forecast(h = nrow(validation_df))

# performance metrics
snaive_performance <- snaive_forecast |>
  accuracy(data = validation_df)

# add results to performance metrics table
performance_metrics <- performance_metrics |>
  add_row(Model = "SNAIVE",
          RMSE = snaive_performance$RMSE,
          MAE = snaive_performance$MAE,
          MAPE = snaive_performance$MAPE
          )
# view table
performance_metrics
```

```{r}
# Visualize
snaive_plot <- autoplot(snaive_forecast, tng_df) + 
  autolayer(validation_df, total_calls) + 
  autolayer(snaive_forecast, color = "red", alpha = 0.25) +
  ggtitle("SNAIVE Forecast v. Validation Data") + 
  labs(y = "Total Calls")

snaive_plot
```

### Auto ARIMA (non-seasonal)

```{r}
# fit the model to the training data
auto_ARIMA <- tng_df |>
  model(ARIMA(total_calls ~ PDQ(0,0,0)) # 0 indicates no seasonal terms
        )

# view the auto selected model parameters
report(auto_ARIMA)
```

```{r}
# forecast the validation period
auto_ARIMA_forecast <- auto_ARIMA |>
  forecast(h = nrow(validation_df))

# performance metrics
AA_perf_metrics <- auto_ARIMA_forecast |> 
  accuracy(data = validation_df)

# add performance metrics to table
performance_metrics <- performance_metrics |> 
  add_row(Model = "Auto Arima",
          RMSE = AA_perf_metrics$RMSE,
          MAE = AA_perf_metrics$MAE,
          MAPE = AA_perf_metrics$MAPE
          )

# view metrics
performance_metrics
```

```{r}
# Visualize
AA_plot <- autoplot(auto_ARIMA_forecast, tng_df) + 
  autolayer(validation_df, total_calls) + 
  autolayer(auto_ARIMA_forecast, color = "red", alpha = 0.25) +
  ggtitle("Auto ARIMA(5,1,1) v. Validation Data") + 
  labs(y = "Total Calls")

AA_plot
```

### Seasonal Auto ARIMA

```{r}
# fit the model to the training data
SAA_Model <- tng_df |>
  model(ARIMA(total_calls))

# view the auto selected model parameters
report(SAA_Model)
```

```{r}
# forecast the validation period
SAA_forecast <- SAA_Model |>
  forecast(h = nrow(validation_df))

# performance metrics
SAA_perf_metrics <- SAA_forecast |> 
  accuracy(data = validation_df)

# add performance metrics to table
performance_metrics <- performance_metrics |> 
  add_row(Model = "Seasonal Auto Arima",
          RMSE = SAA_perf_metrics$RMSE,
          MAE = SAA_perf_metrics$MAE,
          MAPE = SAA_perf_metrics$MAPE
          )

# view metrics
performance_metrics

```

```{r}
# visualize
SAA_plot <- autoplot(SAA_forecast, tng_df) + 
  autolayer(validation_df, total_calls) + 
  autolayer(SAA_forecast, color = "red", alpha = 0.25) +
  ggtitle("Seasonal Auto Arima(1,1,2)(2,0,0)[7] v. Validation Data") + 
  labs(y = "Total Calls")

SAA_plot
```

### Auto ARIMA (w/ Max Temp)

```{r}
# fit the model to the training data
AA_temp_model <- tng_df |>
  model(ARIMA(total_calls ~ PDQ(0,0,0) + 
                TMAX_CEL))

# view the auto selected model parameters
report(AA_temp_model)
```

```{r}
# forecast the validation period
AA_temp_forecast <- AA_temp_model |>
  forecast(new_data = validation_df)

# performance metrics
AA_temp_perf_metrics <- AA_temp_forecast |> 
  accuracy(data = validation_df)

# add performance metrics to table
performance_metrics <- performance_metrics |> 
  add_row(Model = "Auto Arima (w/ Temp)",
          RMSE = AA_temp_perf_metrics$RMSE,
          MAE = AA_temp_perf_metrics$MAE,
          MAPE = AA_temp_perf_metrics$MAPE
          )

# view metrics
performance_metrics
```

```{r}
# visualize
AA_temp_plot <- autoplot(AA_temp_forecast, tng_df) + 
  autolayer(validation_df, total_calls) + 
  autolayer(AA_temp_forecast, color = "red", alpha = 0.25) +
  ggtitle("Auto Arima w/ Max Temp v. Validation Data") + 
  labs(y = "Total Calls")
```

### Seasonal Auto ARIMA (w/ Max Temp)

```{r}
# fit the model to the training data
SAA_temp_model <- tng_df |>
  model(ARIMA(total_calls ~ TMAX_CEL))

# view the auto selected model parameters
report(SAA_temp_model)
```

```{r}
# forecast the validation period
SAA_temp_forecast <- SAA_temp_model |>
  forecast(new_data = validation_df)

# performance metrics
SAA_temp_perf_metrics <- SAA_temp_forecast |> 
  accuracy(data = validation_df)

# add performance metrics to table
performance_metrics <- performance_metrics |> 
  add_row(Model = "Seasonal Auto Arima (w/ Temp)",
          RMSE = SAA_temp_perf_metrics$RMSE,
          MAE = SAA_temp_perf_metrics$MAE,
          MAPE = SAA_temp_perf_metrics$MAPE
          )

# view metrics
performance_metrics
```

```{r}
# visualize
SAA_temp_plot <- autoplot(SAA_temp_forecast, tng_df) + 
  autolayer(validation_df, total_calls) + 
  autolayer(SAA_temp_forecast, color = "red", alpha = 0.25) +
  ggtitle("Seasonal Auto Arima w/ Max Temp v. Validation Data") + 
  labs(y = "Total Calls")
```

## Model Selection

Upon examination of the performance metrics for the selected models it was found that the model with the best performance metrics was the Seasonal Auto ARIMA model which selected parameters of pdq(5,0,0) PDQ(1,0,0) with a period set to 7 days. The model had RMSE = 30.8575 and MAE = 25.3675. These values edge out the Seasonal Auto ARIMA model that includes the max temperature for the day by a small amount with the exception of the performance metric MAPE, which outperformed the SAA model with no temperature 93.8061 compared to 94.1282. Both models performed well, however the model that did not include the maximum temperature for the day has been selected as the model with the highest performance metrics.

### SNAIVE Model Applied to Entire Series

```{r}
# refit pre-trained SNAIVE model to entire dataset
SNAIVE_refit <- snaive_model |>
  refit(model_ts)

# forecast the next 31 days
SNAIVE_final_forecast <- SNAIVE_refit |>
  forecast(h = 31)

# Visualize the forecast
SNAIVE_final_plot <- autoplot(SNAIVE_final_forecast, model_ts) +
  ggtitle("31-Day Forecast using Refitted SNAIVE Model") +
  labs(y = "Total Calls", x = "Date")

# display the plot
SNAIVE_final_plot
```

```{r}
# get November actual values into a df
nov_df <- read.csv(here("datasets/Nov_edify_calls.csv"))

# clean so it is just inbound phone calls
nov_clean <- nov_df |> 
  filter(Communication.Type == "phone") %>%
  filter(Sub.Communication.Type == "inbound")

# drop End.Time, Communication.Type, Sub.Communication.Type
nov_clean <- nov_clean |> 
  select(-End.Time, -Communication.Type, -Sub.Communication.Type)

# get Start.time to just date format
nov_clean <- nov_clean |>
  mutate(Start.Time = as.Date(Start.Time, format = "%m/%d/%Y"))

# get daily counts in a new df with date and nov_calls column
nov_daily_counts <- nov_clean |> 
  group_by(Start.Time) |> 
  summarise(nov_actual = n()) |> 
  arrange(Start.Time) |> 
  rename(Date = Start.Time)

# view
# nov_daily_counts

# merge
nov_results <- full_join(nov_daily_counts, SNAIVE_final_forecast, by = "Date")

# drop unnecessary columns
nov_results <- nov_results |> 
  select(-.model, -total_calls) |>
  mutate(Error = .mean - nov_actual) |> 
  rename(Forecast = .mean)

# produce results
nov_results
```

```{r}
# visualize entirety
final_viz <- ggplot(nov_results, aes(x = Date)) +
  geom_line(aes(y = nov_actual, color = "Actual", linetype = "Actual")) +
  geom_line(aes(y = Forecast, color = "Forecast", linetype = "Forecast")) +
  scale_color_manual(values = c("Actual" = "blue", "Forecast" = "red")) +
  scale_linetype_manual(values = c("Actual" = "solid", "Forecast" = "dashed")) +
  labs(title = "SNAIVE Time Series Monthly Forecast for November",
       x = "Date",
       y = "Calls",
       color = "Legend",
       linetype = "Legend")

final_viz
```
